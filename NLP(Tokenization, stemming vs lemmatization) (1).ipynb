{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddf5de3b-2890-4818-a7a0-f85d0c5a20c0",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d13276c-0b8d-4028-bfcc-121c9bdfac39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\suraj\\anaconda3\\lib\\site-packages (3.6.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\suraj\\anaconda3\\lib\\site-packages (from nltk) (4.60.0)\n",
      "Requirement already satisfied: regex in c:\\users\\suraj\\anaconda3\\lib\\site-packages (from nltk) (2021.4.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\suraj\\anaconda3\\lib\\site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: click in c:\\users\\suraj\\anaconda3\\lib\\site-packages (from nltk) (7.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "111a2723-bfc9-4b34-bc49-4fbee0fb58e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=\"\"\"\n",
    "hi Suraj U r Small dream is data scientist.\n",
    "u r dream comeing soon.\n",
    "within a week u r seleted data scientist position.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "92265ddb-4248-46bf-ad77-be35d098b692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paragraph ---> sentence\n",
    "# sent_tokenize is techniques\n",
    "# sentences working on full stop(.) & exclamation symbol(!)\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c43d59a0-92dd-4311-99ff-285d7aacadf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nhi Suraj U r Small dream is data scientist.',\n",
       " 'u r dream comeing soon.',\n",
       " 'within a week u r seleted data scientist position.']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5466519c-02a8-421c-9558-2411803c4327",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Suraj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d4d80c39-c439-4308-8189-98865dbc54cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## sentences ---> word convertion\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "01c32a12-c6cb-4489-9d46-e9d763dda70b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi',\n",
       " 'Suraj',\n",
       " 'U',\n",
       " 'r',\n",
       " 'Small',\n",
       " 'dream',\n",
       " 'is',\n",
       " 'data',\n",
       " 'scientist',\n",
       " '.',\n",
       " 'u',\n",
       " 'r',\n",
       " 'dream',\n",
       " 'comeing',\n",
       " 'soon',\n",
       " '.',\n",
       " 'within',\n",
       " 'a',\n",
       " 'week',\n",
       " 'u',\n",
       " 'r',\n",
       " 'seleted',\n",
       " 'data',\n",
       " 'scientist',\n",
       " 'position',\n",
       " '.']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f0387ca2-32f3-4e5f-bc2c-571483363fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# anather one technique to convert sentences into word\n",
    "# as compared word_tokenize. wordpunct_tokenize is more accurate or details\n",
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8bd842d8-a89c-4c14-87f8-7778ef9ca197",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi',\n",
       " 'Suraj',\n",
       " 'U',\n",
       " 'r',\n",
       " 'Small',\n",
       " 'dream',\n",
       " 'is',\n",
       " 'data',\n",
       " 'scientist',\n",
       " '.',\n",
       " 'u',\n",
       " 'r',\n",
       " 'dream',\n",
       " 'comeing',\n",
       " 'soon',\n",
       " '.',\n",
       " 'within',\n",
       " 'a',\n",
       " 'week',\n",
       " 'u',\n",
       " 'r',\n",
       " 'seleted',\n",
       " 'data',\n",
       " 'scientist',\n",
       " 'position',\n",
       " '.']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpunct_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "77795926-50d9-48a1-8e45-1489673559d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# treebankword_tokenizer\n",
    "\n",
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "90e1008f-b31c-4278-854d-5a1cbd03c4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "baabfe98-e8c1-4a99-87f6-324cf145b93c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi',\n",
       " 'Suraj',\n",
       " 'U',\n",
       " 'r',\n",
       " 'Small',\n",
       " 'dream',\n",
       " 'is',\n",
       " 'data',\n",
       " 'scientist.',\n",
       " 'u',\n",
       " 'r',\n",
       " 'dream',\n",
       " 'comeing',\n",
       " 'soon.',\n",
       " 'within',\n",
       " 'a',\n",
       " 'week',\n",
       " 'u',\n",
       " 'r',\n",
       " 'seleted',\n",
       " 'data',\n",
       " 'scientist',\n",
       " 'position',\n",
       " '.']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efe5211-abe0-4367-a94d-de2d4581d4b9",
   "metadata": {},
   "source": [
    "# stemming vs Lemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ef2488-6a11-4af6-b06a-904e220318e3",
   "metadata": {},
   "source": [
    "## stemming"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8c4b41e1-e991-41db-a597-70d77a9f8363",
   "metadata": {},
   "source": [
    "Stemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words known as a lemma. Stemming is important in natural language understanding (NLU) and natural language processing (NLP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4d2f6e9-78a0-4afc-a581-e69da5496bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83b0f297-ab0c-44dc-922f-1062c37258dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemming = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdd7a41c-eaf7-4370-95ac-6736f6f5853f",
   "metadata": {},
   "outputs": [],
   "source": [
    "words=[\"eating\",\"eats\",\"eaten\",\"writing\",\"writes\",\"programming\",\"programs\",\"history\",\"finally\",\"finalized\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ecf560b-186c-4052-913c-1f34b2571427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating---->eat\n",
      "eats---->eat\n",
      "eaten---->eaten\n",
      "writing---->write\n",
      "writes---->write\n",
      "programming---->program\n",
      "programs---->program\n",
      "history---->histori\n",
      "finally---->final\n",
      "finalized---->final\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+\"---->\"+stemming.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1928c8c2-2591-4632-a70c-557db9399398",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'congratul'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemming.stem('congratulations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e9b159c-7368-45a2-9815-2a02f9391472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sit'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemming.stem('sitting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5554d7c-964b-4902-921f-dd692b29ee72",
   "metadata": {},
   "outputs": [],
   "source": [
    "### LAncaster  Stemming algorithm\n",
    "from nltk.stem import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2efd6c26-0d53-4808-a37c-9a7491b4b90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lancaster=LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ea70f4d-b878-4775-93cc-8d0f391fd046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating---->eat\n",
      "eats---->eat\n",
      "eaten---->eat\n",
      "writing---->writ\n",
      "writes---->writ\n",
      "programming---->program\n",
      "programs---->program\n",
      "history---->hist\n",
      "finally---->fin\n",
      "finalized---->fin\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+\"---->\"+lancaster.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9dc5f1-d57e-41b3-b1fd-e23b38f80397",
   "metadata": {},
   "source": [
    "## RegexpStemmer class"
   ]
  },
  {
   "cell_type": "raw",
   "id": "73ead342-f882-45bb-90cc-376ce9e65bcd",
   "metadata": {},
   "source": [
    "NLTK has RegexpStemmer class with the help of which we can easily implement Regular Expression Stemmer algorithms. It basically takes a single regular expression and removes any prefix or suffix that matches the expression. Let us see an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7fc7071-d046-4d0f-b7ab-7d8411e68bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import RegexpStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "efc22085-6854-4b98-ac86-cb0e6026c465",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_stemmer=RegexpStemmer('ing|s$|e$|able$', min=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e4de3881-0071-4fbf-bfb8-4658a4b8030e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_stemmer.stem(\"eating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f07b17e-503d-4932-8a20-9b73a255b693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'play'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_stemmer.stem(\"ingplaying\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47ccc24-d6a5-4c7d-a610-45cd5427c673",
   "metadata": {},
   "source": [
    "# Snowball Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a1a42470-3291-49d2-b42e-3b5ed29aa244",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eb15dee5-9e84-48fb-9dda-26b2f558d91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "snowballstemmer=SnowballStemmer('english',ignore_stopwords=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "69240965-9330-432e-bea0-0e69aeac8e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating---->eat\n",
      "eats---->eat\n",
      "eaten---->eaten\n",
      "writing---->write\n",
      "writes---->write\n",
      "programming---->program\n",
      "programs---->program\n",
      "history---->histori\n",
      "finally---->final\n",
      "finalized---->final\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+\"---->\"+snowballstemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d5ae2244-ef22-49a4-92b2-e34c687a3d22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fairli', 'sportingli')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemming.stem(\"fairly\"),stemming.stem(\"sportingly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "deaafb16-57bb-44f7-89f4-bf17fa844586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fair', 'sport')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowballstemmer.stem(\"fairly\"),snowballstemmer.stem(\"sportingly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69a3a92-e902-4576-a919-f07a9ddb7957",
   "metadata": {},
   "source": [
    "# Wordnet Lemmatizer"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7dfdb586-ec10-44a9-931e-6e8b66a0bfa7",
   "metadata": {},
   "source": [
    "Lemmatization technique is like stemming. The output we will get after lemmatization is called ‘lemma’, which is a root word rather than root stem, the output of stemming. After lemmatization, we will be getting a valid word that means the same thing.\n",
    "\n",
    "NLTK provides WordNetLemmatizer class which is a thin wrapper around the wordnet corpus. This class uses morphy() function to the WordNet CorpusReader class to find a lemma. Let us understand it with an example −"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b343a7cb-c518-4494-b495-ad32eacd4c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "df6c5484-ff4a-420d-9c0c-1ac9f41e00c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating---->eat\n",
      "eats---->eat\n",
      "eaten---->eat\n",
      "writing---->write\n",
      "writes---->write\n",
      "programming---->program\n",
      "programs---->program\n",
      "history---->history\n",
      "finally---->finally\n",
      "finalized---->finalize\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "POS- Noun-n\n",
    "verb-v\n",
    "adjective-a\n",
    "adverb-r\n",
    "'''\n",
    "for word in words:\n",
    "    print(word+\"---->\"+lemmatizer.lemmatize(word,pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "907716d3-b9f0-4e7e-8267-f851f6a1cb92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'good'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"good\",pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a2d3463a-19a8-4259-ad0f-ecea7a7c5dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sentiment Analysis-- stemming\n",
    "## Chatbot---lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4160d412-629c-454f-886e-501cbf6ac7a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
